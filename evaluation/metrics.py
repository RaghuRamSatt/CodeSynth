"""
Evaluation Metrics for LLM-Generated Data Analysis Code
"""

import time
import re
import ast
import numpy as np
import pandas as pd
from typing import Dict, Any, List, Tuple, Optional, Union
import logging

logger = logging.getLogger(__name__)

class CodeEvaluator:
    """
    Evaluates code generated by LLM models for data analysis tasks.
    """
    
    def __init__(self, execution_engine=None):
        """
        Initialize the code evaluator.
        
        Args:
            execution_engine: The execution engine to run the code
        """
        self.execution_engine = execution_engine
        # self.current_dataset_path = None

    def evaluate_code(self, code: str, prompt: str, expected_results: Optional[Dict] = None, 
                      dataset: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Evaluate generated code based on multiple metrics.
        
        Args:
            code: The generated Python code
            prompt: The original user prompt
            expected_results: Expected outputs or results (if available)
            dataset: The dataset the code should operate on
            
        Returns:
            Dictionary containing evaluation metrics
        """
        results = {
            "functional_correctness": self.evaluate_functional_correctness(code, dataset),
            "code_quality": self.evaluate_code_quality(code),
            "query_relevance": self.evaluate_query_relevance(code, prompt),
            "execution_metrics": self.evaluate_execution_metrics(code, dataset),
            "visualization_quality": self.evaluate_visualization_quality(code)
        }
        
        # Calculate an overall score (weighted average)
        weights = {
            "functional_correctness": 0.35,
            "code_quality": 0.20,
            "query_relevance": 0.25,
            "execution_metrics": 0.10,
            "visualization_quality": 0.10
        }
        
        # Calculate weighted score for metrics with numerical values
        score_components = []
        weight_sum = 0
        
        for key, weight in weights.items():
            if key in results and isinstance(results[key].get("score"), (int, float)):
                score_components.append(results[key]["score"] * weight)
                weight_sum += weight
        
        if weight_sum > 0:
            results["overall_score"] = sum(score_components) / weight_sum
        else:
            results["overall_score"] = 0
            
        return results
    
    def evaluate_functional_correctness(self, code: str, dataset: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Evaluate if the code executes without errors.
        
        Args:
            code: The generated Python code
            dataset: The dataset the code should operate on
            
        Returns:
            Dictionary with functional correctness metrics
        """
        if not self.execution_engine:
            return {"score": 0, "message": "No execution engine available"}
        
        try:
            # Parse the code to check for syntax errors
            ast.parse(code)
            
            # Get dataset_path from the test_case
            dataset_path = None
            if hasattr(dataset, 'test_case_path'):
                dataset_path = dataset.test_case_path
                
            # Execute the code
            result = self.execution_engine.execute_code(code, dataset_path=dataset_path)
            
            if result["success"]:
                return {
                    "score": 1.0,
                    "message": "Code executed successfully",
                    "execution_result": result
                }
            else:
                return {
                    "score": 0.0,
                    "message": f"Execution failed: {result.get('error', 'Unknown error')}",
                    "execution_result": result
                }
                
        except SyntaxError as e:
            return {
                "score": 0.0,
                "message": f"Syntax error: {str(e)}",
                "error_type": "syntax"
            }
        except Exception as e:
            return {
                "score": 0.0,
                "message": f"Error during evaluation: {str(e)}",
                "error_type": "other"
            }
    
    def evaluate_code_quality(self, code: str) -> Dict[str, Any]:
        """
        Evaluate the quality of the generated code.
        
        Args:
            code: The generated Python code
            
        Returns:
            Dictionary with code quality metrics
        """
        metrics = {}
        
        # Check for comments (documentation)
        comment_lines = len(re.findall(r'^\s*#.*$', code, re.MULTILINE))
        total_lines = len(code.split('\n'))
        metrics["comment_ratio"] = comment_lines / max(total_lines, 1)
        
        # Check for proper imports
        has_imports = bool(re.search(r'^import\s+|^from\s+.*\s+import', code, re.MULTILINE))
        metrics["has_imports"] = has_imports
        
        # Check for error handling
        has_error_handling = 'try:' in code and 'except' in code
        metrics["has_error_handling"] = has_error_handling
        
        # Check for functions or meaningful structure
        has_functions = bool(re.search(r'^def\s+\w+\s*\(', code, re.MULTILINE))
        metrics["has_functions"] = has_functions
        
        # Calculate overall code quality score (0-1)
        # score_components = [
        #     metrics["comment_ratio"] * 0.3,
        #     metrics["has_imports"] * 0.1,
        #     metrics["has_error_handling"] * 0.3,
        #     metrics["has_functions"] * 0.3
        # ]

        score_components = [
        metrics["comment_ratio"] * 0.25,
        metrics["has_imports"] * 0.1,
        metrics["has_error_handling"] * 0.4,  # Increased from 0.3
        metrics["has_functions"] * 0.25       # Reduced from 0.3
        ]
        
        metrics["score"] = sum(score_components)
        
        # Qualitative assessment
        if metrics["score"] >= 0.8:
            metrics["assessment"] = "Excellent"
        elif metrics["score"] >= 0.6:
            metrics["assessment"] = "Good"
        elif metrics["score"] >= 0.4:
            metrics["assessment"] = "Average"
        else:
            metrics["assessment"] = "Poor"
            
        return metrics
    
    def evaluate_query_relevance(self, code: str, prompt: str) -> Dict[str, Any]:
        """
        Evaluate how well the code addresses the user's query.
        
        Args:
            code: The generated Python code
            prompt: The original user prompt
            
        Returns:
            Dictionary with query relevance metrics
        """
        # Extract key terms from the prompt
        prompt_lower = prompt.lower()
        
        # Check for common data analysis tasks
        relevance_checks = {
            "descriptive_stats": any(term in prompt_lower for term in ["average", "mean", "median", "statistics", "std", "min", "max"]),
            "visualization": any(term in prompt_lower for term in ["plot", "chart", "graph", "visualize", "histogram", "scatter"]),
            "correlation": any(term in prompt_lower for term in ["correlation", "relationship", "related", "affects", "impact"]),
            "grouping": any(term in prompt_lower for term in ["group", "category", "segment", "by"]),
            "filtering": any(term in prompt_lower for term in ["filter", "where", "only", "exclude", "include"])
        }
        
        # Check if code addresses the identified tasks
        code_lower = code.lower()
        addressed_tasks = {
            "descriptive_stats": any(term in code_lower for term in ["mean(", "median(", "describe(", "std(", "min(", "max("]),
            "visualization": any(term in code_lower for term in ["plot(", "plt.", "sns.", "figure", "chart", "graph"]),
            "correlation": any(term in code_lower for term in ["corr(", "correlation", "heatmap(", "regplot("]),
            "grouping": any(term in code_lower for term in ["groupby(", "group by"]),
            "filtering": any(term in code_lower for term in ["loc[", "iloc[", "query(", "where(", "mask("])
        }
        
        # Count how many relevant tasks were addressed in the code
        relevant_tasks = sum(1 for task, needed in relevance_checks.items() if needed)
        addressed_relevant_tasks = sum(1 for task, needed in relevance_checks.items() 
                                    if needed and addressed_tasks.get(task, False))
        
        # Calculate relevance score
        relevance_score = addressed_relevant_tasks / max(relevant_tasks, 1)
        
        return {
            "score": relevance_score,
            "relevant_tasks_identified": relevant_tasks,
            "relevant_tasks_addressed": addressed_relevant_tasks,
            "relevance_checks": relevance_checks,
            "addressed_tasks": addressed_tasks
        }
        
    def evaluate_execution_metrics(self, code: str, dataset: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Evaluate execution time and efficiency.
        
        Args:
            code: The generated Python code
            dataset: The dataset the code should operate on
            
        Returns:
            Dictionary with execution metrics
        """
        if not self.execution_engine:
            return {"score": 0, "message": "No execution engine available"}
    
        try:
            # Get dataset_path from the dataset
            dataset_path = None
            if hasattr(dataset, 'test_case_path'):
                dataset_path = dataset.test_case_path
                
            # Measure execution time
            start_time = time.time()
            result = self.execution_engine.execute_code(code, dataset_path=dataset_path)
            execution_time = time.time() - start_time
            
            # Simple scoring based on execution time
            # This is naive; in practice, you'd want to normalize based on task complexity
            if execution_time < 1.0:
                time_score = 1.0
            elif execution_time < 3.0:
                time_score = 0.8
            elif execution_time < 10.0:
                time_score = 0.6
            else:
                time_score = 0.4
                
            return {
                "score": time_score if result["success"] else 0.0,
                "execution_time": execution_time,
                "success": result["success"],
                "error": result.get("error", None)
            }
            
        except Exception as e:
            return {
                "score": 0.0,
                "message": f"Error during execution metrics evaluation: {str(e)}",
                "execution_time": None,
                "success": False
            }
    
    def evaluate_visualization_quality(self, code: str) -> Dict[str, Any]:
        """
        Evaluate the quality of visualizations in the code.
        
        Args:
            code: The generated Python code
            
        Returns:
            Dictionary with visualization quality metrics
        """
        # Check if the code contains visualization libraries
        has_matplotlib = 'matplotlib' in code or 'plt.' in code
        has_seaborn = 'seaborn' in code or 'sns.' in code
        has_plotly = 'plotly' in code
        
        # No visualizations
        if not any([has_matplotlib, has_seaborn, has_plotly]):
            return {
                "score": 0.0,
                "message": "No visualizations found",
                "has_visualizations": False
            }
        
        # Check for best practices in visualizations
        has_titles = bool(re.search(r'\.title\(|\.set_title\(', code))
        has_labels = bool(re.search(r'\.xlabel\(|\.ylabel\(|\.set_xlabel\(|\.set_ylabel\(', code))
        has_legend = 'legend' in code
        has_customization = bool(re.search(r'\.set_style\(|\.set_context\(|\.set_palette\(|\.figure\(', code))
        has_dynamic_layout = bool(re.search(r'subplots\(\s*.*len\(.*\)', code) or 
                        re.search(r'for\s+i\s*,\s*col\s+in\s+enumerate', code) or
                        re.search(r'ncols\s*=\s*len\(', code))
        
        # Calculate score
        viz_score_components = [
            has_titles * 0.25,
            has_labels * 0.25,
            has_legend * 0.15,
            has_customization * 0.15,
            has_dynamic_layout * 0.2  # Give weight to dynamic layouts
        ]
        
        viz_score = sum(viz_score_components)
        
        return {
            "score": viz_score,
            "has_visualizations": True,
            "has_titles": has_titles,
            "has_labels": has_labels,
            "has_legend": has_legend,
            "has_customization": has_customization,
            "has_dynamic_layout": has_dynamic_layout,  # Add this line
            "visualization_libraries": {
                "matplotlib": has_matplotlib,
                "seaborn": has_seaborn,
                "plotly": has_plotly
            }
        }


class TokenEfficiencyEvaluator:
    """
    Evaluates token efficiency of LLM responses.
    """
    
    def calculate_token_efficiency(self, prompt_tokens: int, completion_tokens: int, 
                                  evaluation_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Calculate token efficiency metrics.
        
        Args:
            prompt_tokens: Number of tokens in the prompt
            completion_tokens: Number of tokens in the completion
            evaluation_results: Results from code evaluation
            
        Returns:
            Dictionary with token efficiency metrics
        """
        # Get the overall score from evaluation results
        overall_score = evaluation_results.get("overall_score", 0)
        
        # Calculate efficiency metrics
        total_tokens = prompt_tokens + completion_tokens
        
        # Score per token (higher is better)
        score_per_token = overall_score / max(total_tokens, 1) * 1000
        
        # Prompt efficiency (how much output quality per input token)
        prompt_efficiency = overall_score / max(prompt_tokens, 1) * 100
        
        return {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "score_per_token": score_per_token,
            "prompt_efficiency": prompt_efficiency
        }


class ComprehensiveEvaluator:
    """
    Combines all evaluation metrics into a single evaluation pipeline.
    """
    
    def __init__(self, execution_engine=None):
        """
        Initialize the comprehensive evaluator.
        
        Args:
            execution_engine: The execution engine to run the code
        """
        self.code_evaluator = CodeEvaluator(execution_engine)
        self.token_evaluator = TokenEfficiencyEvaluator()
        
    def evaluate(self, code: str, prompt: str, prompt_tokens: int, completion_tokens: int,
           expected_results: Optional[Dict] = None, dataset: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Run comprehensive evaluation on generated code.
        
        Args:
            code: The generated Python code
            prompt: The original user prompt
            prompt_tokens: Number of tokens in the prompt
            completion_tokens: Number of tokens in the completion
            expected_results: Expected outputs or results (if available)
            dataset: The dataset the code should operate on
            
        Returns:
            Dictionary with all evaluation metrics
        """
        # Extract dataset_path from dataset object if available
        dataset_path = None
        if hasattr(dataset, 'test_case_path'):
            dataset_path = dataset.test_case_path
            
        # Pass dataset_path directly to code_evaluator for cleaner evaluation
        code_eval_results = self.code_evaluator.evaluate_code(code, prompt, expected_results, dataset)
        
        # Evaluate token efficiency
        token_efficiency = self.token_evaluator.calculate_token_efficiency(
            prompt_tokens, completion_tokens, code_eval_results
        )
        
        # Combine results
        results = {
            "code_evaluation": code_eval_results,
            "token_efficiency": token_efficiency,
            "overall_score": code_eval_results.get("overall_score", 0),
            "timestamp": time.time()
        }
        
        return results